{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Chinese character \"detection\"\n",
    "LT2326, Autumn 2021\n",
    "\n",
    "Name: Max Boholm (gusbohom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cotains the code for *Assignment 1: Chinese character \"detection\"* for the course *Machine learning for statistical NLP: Advanced* (course code LT2326), Autumn 2021. The notebook is organized into the folowing parts:\n",
    "\n",
    "*    Meta variables (the term *hyperparameter* is here reserved for decisions on the models), which define ... the loacation (path) of the data, ... \n",
    "*    Data preparation\n",
    "*    Definition and training of two models\n",
    "*    Testing and evaluation\n",
    "*    ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THINK AGAIN ABOUT WHAT THE PROBLEM IS**\n",
    "How do you go from the \"topographic\" representation back to coordinates?\n",
    "Consider using coordinates as targets, how do you calculate the loss? (Perhaps not that hard... one output for every dimension of coordinates... but then: ) How to handle variable length of boxes in image?\n",
    "\n",
    "???\n",
    "predict coordinates\n",
    "for a 2048 x 2048 vector translate those coordinates to 0s and 1s\n",
    "compute loss for that vector\n",
    "\n",
    "\n",
    "\n",
    "*    adjusted box vs polygon proper - how to implement the latter? Cross-secting pixels? Too complicated?!\n",
    "*    Rescaling - how stupid is my approach?\n",
    "*    How to build (convolutional) models?\n",
    "    *    Does I have to consider padding? (If so, perhaps best to use a `torch` dataloader)\n",
    "    *    Flattening\n",
    "*    How to visualize?\n",
    "    *    Reverse flattening\n",
    "    *    Heatmap simple\n",
    "    *    Heatmap-layer\n",
    "*    Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"../../scratch/lt2326-h21/a1/\"\n",
    "path = \"../develop_util/\"\n",
    "images_dir = path + \"images/\"      # the directory of images to be data\n",
    "meta_ctw   = path + \"info.json\"   # the (path to) the general meta file of CTW \n",
    "meta_train = path + \"train.jsonl\" # the (path to) the file containing the annotations of CTW training data\n",
    "\n",
    "train_proportion = 0.7 # the proportion of training data; proportion of test data will be the complement of this number\n",
    "\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "path_to_save_models = \"../models/\"\n",
    "\n",
    "restriction = None # set to an integer, if subsample of the data is to be used in e.g. the development phase\n",
    "rescale_input_to = 100 # ...\n",
    "rescale_output_to = 256 # ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024.0 2\n",
      "256.0 4\n",
      "32.0 8\n",
      "2.0 16\n",
      "0.0625 32\n"
     ]
    }
   ],
   "source": [
    "def devider(n):\n",
    "    numerator = 2\n",
    "    while n > 1:\n",
    "        n = n/numerator\n",
    "        print(n, numerator)\n",
    "        numerator = numerator * 2\n",
    "devider(2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Decision:* batching is kept as list until feeding it to model (`flat_batch`). The list makes the evaluation part simpler as we do not have to bend our minds around multi-dimensional tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_train(file_dir = images_dir, meta_file = meta_ctw):\n",
    "    \"\"\"Tests whether image files in a directory is part of the training set of the CTW dataset, \n",
    "       as defined by the json metafile for CTW. Returns a list of filenames such that they are both\n",
    "       in the specified directory and the CTW training set.\n",
    "    \"\"\"\n",
    "    meta = json.load(open(meta_file,\"r\"))\n",
    "    \n",
    "    train_files_CTW = [entry[\"file_name\"] for entry in meta[\"train\"]]\n",
    "    \n",
    "    files_to_keep = []\n",
    "    \n",
    "    potential_files=[file.split(\"/\")[-1] for file in glob.glob(file_dir+\"*.jpg\")]\n",
    "    \n",
    "    for file in potential_files:\n",
    "        if file in train_files_CTW:\n",
    "            files_to_keep.append(file)\n",
    "            \n",
    "    return files_to_keep\n",
    "\n",
    "def CTW_mapper(files, meta = meta_train):\n",
    "    \"\"\" Identifies annotations for files from the training set of the CTW dataset. \n",
    "        Returns pyhoton dictionary that maps filenames (keys) with annotations (values), \n",
    "        which like in the original format is a list of lists of json elements / python dictinaries. \n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "   \n",
    "    with open(meta, \"r\") as f:\n",
    "        \n",
    "        annotations_data = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "        #print(annotations_data[1])\n",
    "\n",
    "        for file in files:\n",
    "            for annotation in annotations_data:\n",
    "                if annotation[\"image_id\"] == file[:-4]:\n",
    "                    mapping[file] = annotation[\"annotations\"]\n",
    "                    break\n",
    "    \n",
    "    return mapping\n",
    "                    \n",
    "def img2array(file, directory = images_dir, rescale = rescale_input_to):\n",
    "    \"\"\" Takes a filename of an image in a directory and returns a ...\n",
    "    \"\"\"\n",
    "    \n",
    "    img = Image.open(directory+file)\n",
    "    \n",
    "    if rescale != None:\n",
    "        img = img.resize((rescale, rescale))\n",
    "    \n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    return img_np\n",
    "\n",
    "#Standardization is a problem, since it cannot be computed for individual images, but only for the dataset!\n",
    "\n",
    "def coordinates2array(file, mapping, height  = 2048, width   = 2048, rescale = rescale_output_to): \n",
    "    \"\"\" Builds a vector of 0s and 1s based on the coordinates ...\n",
    "    \"\"\"\n",
    "    \n",
    "    if rescale != None:\n",
    "        rescale_by = rescale / height # must come first\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "    \n",
    "    char_matrix = np.zeros((height, width))\n",
    "    \n",
    "    char_areas = []\n",
    "    for block in mapping[file]:\n",
    "        for character in block:\n",
    "            if character[\"is_chinese\"] == True:\n",
    "                xmin = int(character[\"adjusted_bbox\"][0])\n",
    "                ymin = int(character[\"adjusted_bbox\"][1])\n",
    "                w    = int(character[\"adjusted_bbox\"][2])\n",
    "                h    = int(character[\"adjusted_bbox\"][3])\n",
    "                \n",
    "                if rescale != None:\n",
    "                    xmin = int(xmin * rescale_by)\n",
    "                    ymin = int(ymin * rescale_by)\n",
    "                    w    = int(w    * rescale_by)\n",
    "                    h    = int(h    * rescale_by)\n",
    "                \n",
    "                char_areas.append((xmin, ymin, w, h))\n",
    "    \n",
    "    for xmin, ymin, w, h in char_areas:\n",
    "        \n",
    "        r1 = height - ymin - h\n",
    "        r2 = r1 + h\n",
    "        c1 = width - xmin\n",
    "        c2 = c1 + w\n",
    "        \n",
    "        char_matrix[r1:r2 , c1:c2] = 1\n",
    "        \n",
    "    return char_matrix\n",
    "        \n",
    "def data_builder(files, directory, mapping, restriction = restriction):\n",
    "    \"\"\" Compiles the data set for use. Returns a list of dictionaties, specifying:\n",
    "        -  the filename; key: \"file\"\n",
    "        -  the training data (vector); key: \"img_vector\"\n",
    "        -  the labels (a vector of 0s and 1s indicating boxes of characters in images); key: \"label\" \n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file in files: \n",
    "        instance = {}\n",
    "        instance[\"file\"] = file\n",
    "        instance[\"img_vector\"] = img2array(file)\n",
    "        instance[\"label\"] = coordinates2array(file, mapping)\n",
    "        data.append(instance) \n",
    "    \n",
    "    if restriction != None:\n",
    "        random.shuffle(data)\n",
    "        data = data[:restriction]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def standardizer(dataset, scaler = StandardScaler()):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        \n",
    "    std_data = []\n",
    "    #N = len(dataset)\n",
    "    example = dataset[0][\"img_vector\"]\n",
    "    x = example.shape[0]\n",
    "    y = example.shape[1]\n",
    "    z = example.shape[2]\n",
    "    n_features = example.size # ... or x * y * z\n",
    "    \n",
    "    for instance in dataset:\n",
    "        std_data.append(instance[\"img_vector\"].reshape(n_features))\n",
    "    \n",
    "    scaled_data = scaler.fit_transform(std_data)\n",
    "    \n",
    "    for i, scaled_ins in enumerate(scaled_data):\n",
    "        dataset[i][\"img_vector\"] = scaled_ins.reshape(x, y, z)\n",
    "\n",
    "def numpy2torch(dataset, device = device, permute = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of vector before: \", dataset[0][\"img_vector\"].shape)\n",
    "    \n",
    "    for instance in dataset:\n",
    "        if permute == True: # ... hmmm \n",
    "            instance[\"img_vector\"] = torch.Tensor(instance[\"img_vector\"], device = device).permute(2,0,1)\n",
    "        else:\n",
    "            instance[\"img_vector\"] = torch.Tensor(instance[\"img_vector\"], device = device)\n",
    "        instance[\"label\"] = torch.Tensor(instance[\"label\"], device = device)\n",
    "    \n",
    "    print(\"Shape of vector after: \", dataset[0][\"img_vector\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector before:  (100, 100, 3)\n",
      "Shape of vector after:  torch.Size([3, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "files = only_train()\n",
    "mapping = CTW_mapper(files)\n",
    "my_data = data_builder(files, images_dir, mapping)\n",
    "standardizer(my_data)\n",
    "numpy2torch(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data       = my_data, \n",
    "          train_prop = train_proportion, \n",
    "          val_prop   = None):\n",
    "    \n",
    "    if val_prop != None:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        val_to_idx   = int(len(data) * val_prop) + train_to_idx\n",
    "        train = data[:train_to_idx]\n",
    "        val   = data[train_to_idx:val_to_idx]\n",
    "        test  = data[val_to_idx:]\n",
    "        return train, val, test\n",
    "    else:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        train = data[:train_to_idx]\n",
    "        test  = data[train_to_idx:]\n",
    "        return train, test\n",
    "\n",
    "train_set, test_set = split(my_data) #val?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need padding?!\n",
    "# permutation? will affect the tensor --> array --> img \n",
    "\n",
    "#to be used in training below ... \n",
    "def dataloader(data, batch_size=None):\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    if batch_size != None:\n",
    "        for group in [data[i : i+batch_size] for i in range(0, len(data), batch_size)]:\n",
    "            files = []\n",
    "            img_vecs = []\n",
    "            labels = []\n",
    "\n",
    "            for instance in group:\n",
    "                files.append(instance[\"file\"])\n",
    "                img_vecs.append(instance[\"img_vector\"])\n",
    "                labels.append(instance[\"label\"])\n",
    "\n",
    "            batch = {\"file\":files, \n",
    "                     \"img_vector\":img_vecs, \n",
    "                     \"label\":labels}\n",
    "        yield batch\n",
    "\n",
    "    else:\n",
    "        for instance in data:\n",
    "            yield instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "['0000181.jpg', '0000174.jpg']\n",
      "----------\n",
      "['0000188.jpg', '0000189.jpg']\n",
      "----------\n",
      "['0000181.jpg', '0000187.jpg']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"-\"*10)\n",
    "    for j in dataloader(my_data, 4):\n",
    "        print(j[\"file\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(files)\n",
    "#print(mapping)\n",
    "print(my_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 1s\n",
    "labels = [x[\"label\"] for x in my_data]\n",
    "i=0\n",
    "j=1\n",
    "for vec in labels[:3]:\n",
    "    print(j)\n",
    "    for x in vec:\n",
    "        for y in x:\n",
    "            if y != 0:\n",
    "                i = i+1\n",
    "    j+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.zeros(10, 10)\n",
    "t[1:3, 5:7]=1 # rows, columns\n",
    "print(len(t.shape))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((10, 10))\n",
    "a[1:3, 5:7]=1 # rows, columns\n",
    "print(a.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(5,25)\n",
    "print(a.shape)\n",
    "print(a.size)\n",
    "c=np.random.randn(10)\n",
    "print(c.shape)\n",
    "b=a.reshape(125)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "img = Image.open(\"../develop_util/images/0000172.jpg\")\n",
    "\n",
    "img_np=np.array(img)\n",
    "img_vec=torch.LongTensor(img_np)\n",
    "#print(img)\n",
    "img_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing flattening and stacking\n",
    "import torch\n",
    "A=[[1,2,3], [4,5,6], [7,8,9]] #three pixel img\n",
    "B=[[10,11,12], [13,14,15], [16,17,18]] #another one\n",
    "T1=torch.tensor(A)\n",
    "T2=torch.tensor(B)\n",
    "C=[T1, T2] # a batch\n",
    "T3=torch.stack([torch.flatten(x) for x in C])\n",
    "print(\"Before:\")\n",
    "print(C)\n",
    "print(\"After:\")\n",
    "print(T3.shape)\n",
    "print(T3)\n",
    "\n",
    "print(T3.size()[0]*T3.size()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_batch(batch):\n",
    "    \"\"\"Takes a list of length B of more-than-one dimensional tensors (N, M, ...) and returns a tensor of shape:\n",
    "    (B, M*N*...)...\"\"\"\n",
    "    \n",
    "    return torch.stack([torch.flatten(instance) for instance in batch])\n",
    "\n",
    "# do we need device assignation here?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            train_data = train_set,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            path_for_saving_model = \"models/\",\n",
    "            my_optimizer = optim.Adam,\n",
    "            my_loss_function = nn.BCELoss()):\n",
    "    \n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    #model = my_model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = my_loss_function\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        iterator = dataloader(train_set, batch_size)\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            output = model(flat_batch(batch[\"img_vector\"]))\n",
    "            targets = flat_batch(batch[\"label\"])\n",
    "            \n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            total_loss += loss.item() \n",
    "            print(\"Epoch: \", epoch+1, \"Batch: \", i, \"Total loss: \", total_loss/(i+1), end='\\r') \n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "        print()\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            \n",
    "            # DO SOMETHING\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, path_for_saving_model+name_of_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, inp, hidden, outp):  \n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(inp, hidden)\n",
    "        self.classifier = nn.Linear(hidden, outp)\n",
    "  \n",
    "    def forward(self, batch): #batch is list :( Covert a list of vectors to a Plus 1 dim vector... Consider Asaads solution\n",
    "        \n",
    "        #flat = flat_batch(batch)\n",
    "        \n",
    "        compression = F.relu(self.layer1(batch))\n",
    "        \n",
    "        output = torch.sigmoid(self.classifier(compression))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 100])\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "shape_of_input = my_data[0][\"img_vector\"].shape\n",
    "input_size = shape_of_input[0] * shape_of_input[1] * shape_of_input[2]\n",
    "shape_of_output = my_data[0][\"label\"].shape\n",
    "output_size = shape_of_output[0] * shape_of_output[1]\n",
    "\n",
    "print(shape_of_input)\n",
    "print(shape_of_output)\n",
    "\n",
    "my_simple_model = SimpleModel(inp=input_size, hidden=100, outp=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SimpleModel(\n",
       "  (layer1): Linear(in_features=30000, out_features=100, bias=True)\n",
       "  (classifier): Linear(in_features=100, out_features=65536, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_simple_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Batch:  0 Total loss:  0.7011890411376953\n",
      "Epoch:  2 Batch:  0 Total loss:  17.661561965942383\n"
     ]
    }
   ],
   "source": [
    "trainer(my_simple_model, # Must be an instance of a model!\n",
    "        \"mamma mu\",\n",
    "        0.005,\n",
    "        2,\n",
    "        2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size_m1 = 1\n",
    "lr_m1 = 0.01 \n",
    "stride_m1 = 1\n",
    "window_m1 = (1,1)\n",
    "m1_name = \"m1_{}{}{}\".format(\"B\", \"S\", \"X\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size_m2 = 1\n",
    "lr_m2 = 0.01 \n",
    "stride_m2 = 1\n",
    "window_m2 = (1,1)\n",
    "m2_name = \"m2_{}{}{}\".format(\"B\", \"S\", \"X\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch\n",
    "stride\n",
    "iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: performance of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, loader = dataloader, threshold = 0.5):\n",
    "    model.eval()\n",
    "\n",
    "    for instance in loader(test_set):\n",
    "        \n",
    "        prediction = model(torch.flatten(instance[\"img_vector\"]))\n",
    "        truth = torch.flatten(instance[\"label\"]).int()\n",
    "        #print(truth)\n",
    "\n",
    "        mse = F.mse_loss(prediction, truth)\n",
    "        \n",
    "        roundof = (prediction >= threshold).int()\n",
    "        \n",
    "        #print(roundof * truth)\n",
    "        \n",
    "        tp = sum(roundof * truth)\n",
    "        #print(\"TP: \", tp)\n",
    "        fp = sum(roundof * (~truth.bool()).float())\n",
    "        #print(\"FP: \", fp)\n",
    "        tn = sum((~roundof.bool()).float() * (~truth.bool()).float())\n",
    "        #print(\"TN: \", tn)\n",
    "        fn = sum((~roundof.bool()).float() * truth)\n",
    "        #print(\"FN: \", fn)\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        recall = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        f1 = (2 * recall * precision) / (recall + precision)\n",
    "        \n",
    "        print(instance[\"file\"])\n",
    "        print(\"MSE: \", mse.item())\n",
    "        print(\"Accuracy: \", accuracy.item())\n",
    "        print(\"Recall: \", recall.item())\n",
    "        print(\"Precision: \", precision.item())\n",
    "        print(\"F1: \", f1.item())\n",
    "        \n",
    "        print(\"-\"*15)\n",
    "        #break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000187.jpg\n",
      "MSE:  0.40677765011787415\n",
      "Accuracy:  0.57037353515625\n",
      "Recall:  0.4095238149166107\n",
      "Precision:  0.0015282368985936046\n",
      "F1:  0.0030451102647930384\n",
      "---------------\n",
      "0000189.jpg\n",
      "MSE:  0.3939460813999176\n",
      "Accuracy:  0.573760986328125\n",
      "Recall:  0.41911765933036804\n",
      "Precision:  0.002042132429778576\n",
      "F1:  0.004064460750669241\n",
      "---------------\n",
      "0000176.jpg\n",
      "MSE:  0.15138916671276093\n",
      "Accuracy:  0.8443756103515625\n",
      "Recall:  0.2293577939271927\n",
      "Precision:  0.004959825426340103\n",
      "F1:  0.009709681384265423\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "evaluator(my_simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Chinese character \"detection\"\n",
    "LT2326, Autumn 2021\n",
    "\n",
    "Name: Max Boholm (gusbohom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cotains the code for *Assignment 1: Chinese character \"detection\"* for the course *Machine learning for statistical NLP: Advanced* (course code LT2326), Autumn 2021. The notebook is organized into the folowing parts:\n",
    "\n",
    "*    Libraries\n",
    "*    Meta variables (the term *hyperparameter* is here reserved for decisions on the models), which define ... the loacation (path) of the data, ... \n",
    "*    Data preparation\n",
    "*    Definition and training of two models\n",
    "*    Testing and evaluation\n",
    "*    ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "import operator\n",
    "import time\n",
    "from math import sqrt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.path as mplpath\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../../../scratch/lt2326-h21/a1/\"\n",
    "#path = \"../develop_util/\"\n",
    "images_dir = path + \"images/\"      # the directory of images to be data\n",
    "meta_ctw   = path + \"info.json\"   # the (path to) the general meta file of CTW \n",
    "meta_train = path + \"train.jsonl\" # the (path to) the file containing the annotations of CTW training data\n",
    "\n",
    "#shortcut_to_prepared_data = \"prepared_data.pkl\"\n",
    "\n",
    "train_proportion = 0.7 # the proportion of training data; proportion of test data will be the complement of this number\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "path_to_save_models = \"../models/\"\n",
    "path_to_save_evaluations = \"../evals/\"\n",
    "\n",
    "# For development purposes\n",
    "restriction = None # set to an integer, if subsample of the data is to be used in e.g. the development phase\n",
    "rescale_input_to = None # preferably choose rescaling from [1024, 512, 256]\n",
    "rescale_output_to = 1024\n",
    "\n",
    "###############################################################\n",
    "#                                                             #\n",
    "#  Note on rescaling: processes which are not convolutional   #\n",
    "#  and handles 2048 x 2048 matrices are very time consuming   #\n",
    "#  (e.g. converting polygon coordinates to 0s-and-1s matrices #\n",
    "#  and finding true positives, true negatives, etc.).         #\n",
    "#  Therefore a good compromise between using full-scale and   #\n",
    "#  non-scaled data is to make the output smaller, while       #\n",
    "#  keeping the original size of the input (non-convolutional  #\n",
    "#  processes ar performed on output and target matrices).     #\n",
    "#                                                             #\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Decision:* batching is kept as python `list` until feeding it to the model. *This means that the training procedure, e.g. the models, must convert the batces to tensors.* For models with \"linear input\", the function `flat_batch` is an example which helps us to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_train(file_dir = images_dir, meta_file = meta_ctw):\n",
    "    \"\"\"Tests whether image files in a directory is part of the training set of the CTW dataset, \n",
    "       as defined by the json metafile for CTW. Returns a list of filenames such that they are both\n",
    "       in the specified directory and the CTW training set.\n",
    "    \"\"\"\n",
    "    meta = json.load(open(meta_file,\"r\"))\n",
    "    train_files_CTW = [entry[\"file_name\"] for entry in meta[\"train\"]]\n",
    "    files_to_keep = []\n",
    "    potential_files=[file.split(\"/\")[-1] for file in glob.glob(file_dir+\"*.jpg\")]\n",
    "    print(\"Potential files: \", len(potential_files))\n",
    "    \n",
    "    for file in potential_files:\n",
    "        if file in train_files_CTW:\n",
    "            files_to_keep.append(file)\n",
    "    \n",
    "    print(\"Kept files: \", len(files_to_keep))\n",
    "    \n",
    "    return files_to_keep\n",
    "\n",
    "def shorty(files, restriction = restriction):\n",
    "    \"\"\" Returns a shorter list of files picked by random.\n",
    "    \"\"\"\n",
    "    \n",
    "    if restriction != None:\n",
    "        print(\"Warning: Now you have made a restriction to \", restriction, \" files.\")\n",
    "        random.shuffle(files)\n",
    "        r_files = files[:restriction]\n",
    "    else:\n",
    "        print(\"No restriction was made, since it was not specified.\")\n",
    "        r_files = files\n",
    "    \n",
    "    return r_files\n",
    "        \n",
    "def CTW_mapper(files, meta = meta_train):\n",
    "    \"\"\" Identifies annotations for files from the training set of the CTW dataset. \n",
    "        Returns pyhoton dictionary that maps filenames (keys) with annotations (values), \n",
    "        which like in the original format is a list of lists of json elements / python dictinaries. \n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    with open(meta, \"r\") as f:\n",
    "        annotations_data = [json.loads(line) for line in f.readlines()]\n",
    "        for file in files:\n",
    "            for annotation in annotations_data:\n",
    "                if annotation[\"image_id\"] == file[:-4]:\n",
    "                    mapping[file] = annotation[\"annotations\"]\n",
    "                    break\n",
    "    \n",
    "    return mapping\n",
    "                    \n",
    "def img2array(file, directory = images_dir, rescale = rescale_input_to):\n",
    "    \"\"\" Takes a filename of an image in a directory and returns an numpy array \n",
    "        corresponding to the image.\n",
    "    \"\"\"\n",
    "    img = Image.open(directory+file)\n",
    "    \n",
    "    if rescale != None:\n",
    "        img = img.resize((rescale, rescale))\n",
    "    \n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    return img_np\n",
    "\n",
    "def grid(height = 2048, width = 2048, rescale = rescale_output_to):\n",
    "    \"\"\" Creates a grid (an array of coordinates) to be used in polygon2array \n",
    "        to speed things up.\n",
    "    \"\"\"\n",
    "    \n",
    "    if rescale != None:\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "        \n",
    "    grid = np.array([[[h,w] for h in list(range(height))] for w in list(range(width))]).reshape(height*width, 2)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def polygon2array(file, mapping, grid, height = 2048, width = 2048, rescale = rescale_output_to):\n",
    "    \"\"\" Builds a matrix of 0s and 1s representing the character polygons as \n",
    "        defined by the coordinates of in the CTW annotations. \n",
    "    \"\"\"\n",
    "    \n",
    "    # This takes long time :(\n",
    "    \n",
    "    polygons = []\n",
    "    for block in mapping[file]: # mapping maps files with their annotations\n",
    "        for character in block:\n",
    "            if character[\"is_chinese\"] == True:\n",
    "                polygons.append(character[\"polygon\"])\n",
    "    #print(\"found all polygons\")\n",
    "    \n",
    "    if rescale != None:\n",
    "        rescale_by = rescale / height # must come first\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "        polygons = [[[point * rescale_by for point in points] for points in set_of_points] for set_of_points in polygons]\n",
    "    \n",
    "    #print(\"start creation grid\")\n",
    "    \n",
    "    every_point = grid.copy()\n",
    "    \n",
    "    #print(\"created every point\")\n",
    "    zeros_to_update = np.zeros(height * width)\n",
    "    \n",
    "    #print(\"start iteration over polygons\")\n",
    "    for polygon in polygons:\n",
    "        path = mplpath.Path(np.array(polygon)) # can this be \"sent\" to Path in one go\n",
    "        #print(\"created path\")\n",
    "        hits = np.asarray(path.contains_points(every_point), int)\n",
    "        #print(\"identified hits\")\n",
    "        zeros_to_update += hits\n",
    "        #print(\"updated zero_matrix\")\n",
    "        #print(\"one polgon down\")\n",
    "    \n",
    "    matrix = zeros_to_update.reshape(height, width)\n",
    "    \n",
    "    #print(np.sum(matrix))\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def data_builder(files, directory, mapping):\n",
    "    \"\"\" Compiles the dataset for use. Returns a list of dictionaties, such that each\n",
    "        element of the list contains:\n",
    "        -  the filename; key: \"file\"\n",
    "        -  a vectorized instance of the training data; key: \"img_vector\"\n",
    "        -  a vectorized instance of the labels, or targets (a vector of 0s and 1s indicating \n",
    "           boxes of characters in images); key: \"label\"\n",
    "        \n",
    "        Note: (1) the format of instances (training input and targets) are numpy arrays; and\n",
    "        (2) the instances have \"matrix shape\". For these reasons, the output of the data_builder()\n",
    "        requires further processing for it to be ready for pytorch processing. The functions \n",
    "        standardizer() and numpy2torch() is does required further down the pipline of data\n",
    "        preprocessing.\n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    my_grid = grid()\n",
    "    data = []\n",
    "    i=1 # for printing out progress\n",
    "    for file in files: \n",
    "        #print(\"NEW FILE\")\n",
    "        instance = {}\n",
    "        instance[\"file\"] = file\n",
    "        instance[\"img_vector\"] = img2array(file)\n",
    "        #print(\"img2array done\")\n",
    "        #instance[\"label\"] = bbox2array(file, mapping)\n",
    "        instance[\"label\"] = polygon2array(file, mapping, grid = my_grid)\n",
    "        #print(\"polygon2array done\")\n",
    "        data.append(instance)\n",
    "        print(\"{}% done.\".format(round((i/len(files))*100, 1)), end=\"\\r\")\n",
    "        i+=1\n",
    "        \n",
    "        #break\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    return data\n",
    "\n",
    "def standardizer(dataset, scaler = StandardScaler()):\n",
    "    \"\"\" Standardizes the image vectors of a dataset to z-scores using StandardScaler() \n",
    "        from the library sklearn.preprocessing. \n",
    "    \"\"\"\n",
    "    \n",
    "    std_data = []\n",
    "    #N = len(dataset)\n",
    "    example = dataset[0][\"img_vector\"]\n",
    "    x, y, z = example.shape\n",
    "    n_features = example.size # ... or x * y * z\n",
    "    \n",
    "    for instance in dataset:\n",
    "        std_data.append(instance[\"img_vector\"].reshape(n_features))\n",
    "    \n",
    "    scaled_data = scaler.fit_transform(std_data)\n",
    "    \n",
    "    for i, scaled_ins in enumerate(scaled_data):\n",
    "        dataset[i][\"img_vector\"] = scaled_ins.reshape(x, y, z)\n",
    "\n",
    "def numpy2torch(dataset, device = device, permute = True):\n",
    "    \"\"\" For a dataset, transforms its numpy arrays to torch tensors. If permute = True,\n",
    "        image vectors are permuted such that ... \n",
    "    \"\"\"\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    for instance in dataset:\n",
    "        if permute == True: # ... hmmm \n",
    "            instance[\"img_vector\"] = torch.tensor(instance[\"img_vector\"], dtype=torch.float, device = device).permute(2,0,1)\n",
    "        else:\n",
    "            instance[\"img_vector\"] = torch.tensor(instance[\"img_vector\"], dtype=torch.float, device = device)\n",
    "        instance[\"label\"] = torch.tensor(instance[\"label\"], dtype=torch.float, device = device)\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling functions: creating the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential files:  1000\n",
      "Kept files:  845\n"
     ]
    }
   ],
   "source": [
    "files = only_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No restriction was made, since it was not specified.\n"
     ]
    }
   ],
   "source": [
    "files = shorty(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = CTW_mapper(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! (16 m., 25 s.)\n"
     ]
    }
   ],
   "source": [
    "my_data = data_builder(files, images_dir, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numpy2torch(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data       = my_data, \n",
    "          train_prop = train_proportion, \n",
    "          val_prop   = None):\n",
    "    \"\"\" Splits a dataset into training data, testing data and, if selected,\n",
    "        validation data. Note that the proportions of training data, test data\n",
    "        and validation data (optional) must not exceed 100%. \n",
    "    \"\"\"\n",
    "    \n",
    "    if val_prop != None:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        val_to_idx   = int(len(data) * val_prop) + train_to_idx\n",
    "        train = data[:train_to_idx]\n",
    "        val   = data[train_to_idx:val_to_idx]\n",
    "        test  = data[val_to_idx:]\n",
    "        return train, val, test\n",
    "    else:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        train = data[:train_to_idx]\n",
    "        test  = data[train_to_idx:]\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split() # there is no validation set used below... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a dataloader\n",
    "In training of models, the function `dataloader` will be called with the `train_set` in every iteration (epoch) yielding  randomized and batched traing inputs. Note that the `dataloader` has a python `list` as basis for iteration, which requires special attention further down in the models and in the training loop in order to provide models with `pytorch` *tensors*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(data, batch_size):\n",
    "    \"\"\" Takes a (proportion of) a dataset and returns a randomized iterator \n",
    "        of the data organized into batches as defined by batch_size.\n",
    "        \n",
    "        Note: the dataloader preserves the \"matrix shape\" of trainingdata and \n",
    "        targets. Since pytorch neural networks require \"flat\" shapes of data\n",
    "        the function flat_batch() is used to let data flow trough training\n",
    "        in the desired format. \n",
    "    \"\"\"\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    for group in [data[i : i+batch_size] for i in range(0, len(data), batch_size)]:\n",
    "        files = []\n",
    "        img_vecs = []\n",
    "        labels = []\n",
    "\n",
    "        for instance in group:\n",
    "            files.append(instance[\"file\"])\n",
    "            img_vecs.append(instance[\"img_vector\"])\n",
    "            labels.append(instance[\"label\"])\n",
    "\n",
    "        batch = {\"file\":files, \n",
    "                 \"img_vector\":img_vecs, \n",
    "                 \"label\":labels}\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data[0][\"label\"].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_batch(batch):\n",
    "    \"\"\"Takes a python list of length B of more-than-one dimensional tensors (N, M, ...) and \n",
    "    returns a tensor of shape: (B, M*N*...). \"\"\"\n",
    "    \n",
    "    return torch.stack([torch.flatten(instance) for instance in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            train_data = train_set,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            directory = path_to_save_models,\n",
    "            #my_loss_function = nn.MSELoss,\n",
    "            my_loss_function = nn.BCELoss,\n",
    "            my_optimizer = optim.Adam\n",
    "           ):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    #model = my_model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = my_loss_function()\n",
    "    \n",
    "    #total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        iterator = dataloader(train_set, batch_size)\n",
    "        for i, batch in enumerate(iterator):\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "            output = model(batch[\"img_vector\"])\n",
    "            targets = flat_batch(batch[\"label\"])\n",
    "            \n",
    "            #print(\"output\", output)\n",
    "            #print(\"targets\", targets)\n",
    "            \n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            #total_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            #print(\"Epoch: \", epoch+1, \"Batch: \", i, \"Total loss: \", total_loss/(i+1), end='\\r')\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            \n",
    "            #break\n",
    "            \n",
    "        #print()\n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            # Here we could do some evaluation of model progress, but I have ignored this for now. \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, directory+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Convolutional Model with Upsampling (CUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUP(nn.Module):\n",
    "    def __init__(self, inp_height, inp_width, ch, outp):   \n",
    "        super(CUP, self).__init__()\n",
    "        \n",
    "        self.height = inp_height\n",
    "        self.width = inp_width\n",
    "        self.channels = ch\n",
    "        \n",
    "        self.output = outp\n",
    "\n",
    "        #self.compression = nn.MaxPool2d(16, 16)\n",
    "\n",
    "        self.compression = nn.Sequential(      # convolution0, a major size reduction\n",
    "            nn.Conv2d(in_channels = 3, \n",
    "                      out_channels = 3, \n",
    "                      kernel_size = 4, \n",
    "                      stride = 4, \n",
    "                      padding = 0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.RReLU(),\n",
    "            nn.MaxPool2d(4, 4)\n",
    "        )\n",
    "        \n",
    "        self.convolution1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3,     # 3 channels in (RGB\n",
    "                      out_channels = self.channels[0], \n",
    "                      kernel_size = 4,     # window of 4x4\n",
    "                      stride = 1, \n",
    "                      padding = \"same\"), \n",
    "            nn.RReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 2x2 window with stride of 2\n",
    "        )\n",
    "        \n",
    "        self.convolution2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = self.channels[0],     \n",
    "                      out_channels = self.channels[1], \n",
    "                      kernel_size = 2,     \n",
    "                      stride = 1, \n",
    "                      padding = \"same\"), \n",
    "            nn.RReLU(),\n",
    "            nn.MaxPool2d(2, 2), # 2x2 window with stride of 2\n",
    "        )\n",
    "        \n",
    "        # Example with 2048 x 2048 images:\n",
    "        # [batch_n, 3, 2048, 2048] --> [batch_n, 3, 128, 128] (compression by convolution and max pooling)\n",
    "        # [batch_n, 3, 128,  128]  --> [batch_n, 32, 64,  64]  (convolution1)\n",
    "        # [batch_n, 32, 64,   64]  --> [batch_n, 64, 32,  32]  (convolution2)\n",
    "        # 64*32*32 = 65536\n",
    "        # 2048 * 2048 (= 4194304 i.e. target_size) / 65536 = 64 (i.e. the number that our tensor need to be upsampled by)\n",
    "        # sqrt(64) = 8 (the upsampling will be applied to both heigt and weigth)\n",
    "        \n",
    "        self.up_factor = sqrt(self.output/(self.channels[1]*((self.height / (16*2*2))**2)))\n",
    "        \n",
    "        self.exit = nn.Sequential(\n",
    "            nn.Dropout(0.10), #why 0.05? higher?\n",
    "            nn.Upsample(scale_factor=self.up_factor, mode='bilinear'), #decide on mode\n",
    "            nn.Flatten(), # by default start_dim = 1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        if isinstance(batch, list):    # this is admittedly a bit ad hoc, but it handles the format of the batched training data and the non-batched evaluation data; and it works ;)\n",
    "            batch = torch.stack(batch) \n",
    "        else:\n",
    "            batch = torch.stack([batch])\n",
    "        \n",
    "        #print(batch.shape)\n",
    "        features0 = self.compression(batch)\n",
    "        #print(\"f0\", features0)\n",
    "        \n",
    "        features1 = self.convolution1(features0)\n",
    "        #print(\"f1\", features1)\n",
    "        #print(\"1\", features1.shape)\n",
    "        \n",
    "        features2 = self.convolution2(features1)\n",
    "        #print(\"f3\", features2)\n",
    "        #print(\"larger than 1\", torch.sum(features2 > 1))\n",
    "        #print(\"smaller than 0\", torch.sum(features2 < 0))\n",
    "        \n",
    "        #print(\"2\", features2.shape)\n",
    "        #print(\"factor\", self.up_factor)\n",
    "        \n",
    "        output = self.exit(features2)\n",
    "        #print(\"out\",  output)\n",
    "        #print(\"    sum_out\", torch.sum(output))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = my_data[0][\"img_vector\"].shape\n",
    "height = shape_of_input[1]\n",
    "width = shape_of_input[2]\n",
    "shape_of_output = my_data[0][\"label\"].shape\n",
    "output_size = shape_of_output[0] * shape_of_output[1]\n",
    "\n",
    "# print(shape_of_input)\n",
    "# print(shape_of_output)\n",
    "print(output_size)\n",
    "\n",
    "my_cup_model = CUP(inp_height = height, \n",
    "                   inp_width = width, \n",
    "                   ch = (32,64), #for [2048, 1024, 512, 256] images choose channel[1] as one of [4, 16, 64]\n",
    "                   outp = output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cup_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "batchsz = 128\n",
    "\n",
    "sample = len(my_data)  \n",
    "inp_rsc = 2048 if rescale_input_to != None else rescale_input_to\n",
    "out_rsc = 2048 if rescale_output_to != None else rescale_output_to\n",
    "\n",
    "cup_name = f\"CUP{inp_rsc}to{out_rsc}_e{nepochs}b{batchsz}_n{sample}\"\n",
    "\n",
    "trainer(my_cup_model, # Must be an instance of a model!\n",
    "        cup_name,\n",
    "        learning_rate=0.001,\n",
    "        epochs=nepochs,\n",
    "        batch_size=batchsz,\n",
    "        save_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Convolutional Diabolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the inspiration for this model was found here: \n",
    "# https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n",
    "\n",
    "class Diabolo(nn.Module):\n",
    "    def __init__(self, inp_dim, outp):   \n",
    "        super(Diabolo, self).__init__()\n",
    "        \n",
    "        self.input_dimension = inp_dim\n",
    "        self.output = outp\n",
    "\n",
    "        self.encoder = nn.Sequential(         # C1\n",
    "            nn.Conv2d(in_channels = 3, \n",
    "                      out_channels = 8, \n",
    "                      kernel_size = 16, \n",
    "                      stride = 8, \n",
    "                      padding = 4),      \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(8, 16, 3, 1, padding = \"same\"), # C2\n",
    "            nn.BatchNorm2d(16), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16, 32, 3, 1, padding = \"same\"),  # C3\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2,2))\n",
    "        \n",
    "        # Encoded size: [n_batch, channels[-1], (heigth / x), (width / x)], \n",
    "        # where x = compression from C1 * compression from C2 * compression from C3,\n",
    "        # which equals: 8 * 2 * 2 * 2. For calculating the compression of C1 we can\n",
    "        # use the formula: \n",
    "        #                         (input dimension + 2*padding - kernel size)\n",
    "        #     output dimension =  ______________________________________________ + 1\n",
    "        #                                        stride\n",
    "        #   \n",
    "        # With the parameters of C1, input dimension dived by output dimension equals a \n",
    "        # a reduction by 8; which is then followed by three 2x2 maxpoolings with stride\n",
    "        # of 2 (i.e. 2**3). For example, for an input of 2048, we get: 2048 (C1)>>> \n",
    "        # ((2048 + 2*8 - 16) / 7 )+1 = 256 (max pool)>>> 128 (max pool)>>> 64 (max pool)>>> 32\n",
    "        # Thus, for input sized 2048 x 2048, encoded size will be: [n_batch, 64, 32, 32].\n",
    "        \n",
    "        self.encoded_dim = self.input_dimension / ((self.input_dimension/((self.input_dimension + 2*4 - 16) / 8)+1) * 2**3)\n",
    "\n",
    "            # written out in full here, but can be simplified substantially, \n",
    "            # see example above: 8 * 2 * 2 * 2\n",
    "\n",
    "        self.factor      = (sqrt(self.output)/self.encoded_dim)/32\n",
    "        \n",
    "        # This manouver to define a factor relative the input and output enables us \n",
    "        # to streamline the model such that it can handle different inputs and outputs. \n",
    "        # As defined below, the first layer of the decoder, outputs the SAME shape from\n",
    "        # different dimensions of the encoded representation (eg. 32, 16, 8). However, \n",
    "        # from a machine learning perspective this approach is admittedly ad hoc. There \n",
    "        # little theoretical motivation for why we should use larger kernels, strides \n",
    "        # and padding for smaller inputs.\n",
    "        \n",
    "        self.decoder = nn.Sequential(                            # D1\n",
    "            nn.ConvTranspose2d(in_channels = 32, \n",
    "                               out_channels = 16, \n",
    "                               kernel_size = int(4*self.factor), \n",
    "                               stride = int(2*self.factor), \n",
    "                               padding = int(1*self.factor)), \n",
    "            \n",
    "#             nn.BatchNorm2d(12), \n",
    "#             nn.ConvTranspose2d(12, 8, 8, 4, padding=2), # D2            \n",
    "#             nn.BatchNorm2d(8),\n",
    "#             nn.ConvTranspose2d(8, 1, 8, 4, padding=2), # D3\n",
    "#             nn.Flatten(),\n",
    "#             nn.Sigmoid())\n",
    "            \n",
    "            \n",
    "            nn.BatchNorm2d(16), \n",
    "            nn.ConvTranspose2d(16, 8, 4, 2, padding=1), # D2\n",
    "            nn.BatchNorm2d(8),            \n",
    "            nn.ConvTranspose2d(8, 3, 4, 2, padding=1), # D3\n",
    "            nn.BatchNorm2d(3), \n",
    "            nn.ConvTranspose2d(3, 1, 4, 2, padding=1), # D4\n",
    "            nn.ConvTranspose2d(1, 1, 4, 2, padding=1), # D5; dimension to aim for is sqrt(self.output)\n",
    "            nn.Flatten(), \n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        if isinstance(batch, list):    # this is admittedly a bit ad hoc, but it handles the format of the batched training data and the non-batched evaluation data; and it works ;)\n",
    "            batch = torch.stack(batch) \n",
    "        else:\n",
    "            batch = torch.stack([batch])\n",
    "            \n",
    "        encoded = self.encoder(batch)\n",
    "        #print(encoded.shape)\n",
    "        output  = self.decoder(encoded)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = my_data[0][\"img_vector\"].shape\n",
    "input_dimension = shape_of_input[1]\n",
    "shape_of_output = my_data[0][\"label\"].shape\n",
    "output_size = shape_of_output[0] * shape_of_output[1]\n",
    "\n",
    "# print(shape_of_input)\n",
    "# print(shape_of_output)\n",
    "print(output_size)\n",
    "print(input_dimension)\n",
    "\n",
    "my_diabolo_model = Diabolo(input_dimension, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_diabolo_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "batchsz = 128\n",
    "\n",
    "sample = len(my_data)  \n",
    "inp_rsc = 2048 if rescale_input_to != None else rescale_input_to\n",
    "out_rsc = 2048 if rescale_output_to != None else rescale_output_to\n",
    "\n",
    "diab_name = f\"Diabolo{inp_rsc}to{out_rsc}_e{nepochs}b{batchsz}_n{sample}\"\n",
    "\n",
    "trainer(my_diabolo_model, # Must be an instance of a model!\n",
    "        diab_name,\n",
    "        learning_rate=0.001,\n",
    "        epochs=nepochs,\n",
    "        batch_size=batchsz,\n",
    "        save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula: [(n+2p-f)/s+1] X [(n+2p-f)/s+1]\n",
    "\n",
    "kernel = 16  #16\n",
    "padding = 4  #\n",
    "stride = 8   #\n",
    "\n",
    "for x in [2048, 1024, 256]:\n",
    "    y = (((x+(2*padding))-kernel) / stride)\n",
    "    z = y + 1\n",
    "    print(x, \">>>\", y, z, \"factor\", x/y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in [2048, 1024, 256, 128, 32]:\n",
    "    print(x, \">>>\", ((x+(2*3))-6) / (3+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [512, 256, 128, 64, 32]:\n",
    "    print(x, \">>>\", ((x+(2*3))-7) / (4+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in [512, 256, 128, 64, 32]:\n",
    "    print(x, \">>>\", x/(2*2*2)) # 3 max poolings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# Conv2D transpose output shape\n",
    "# out_dim = stride * (in_dim - 1) + kernal - 2 * padding\n",
    "            # out_dim = in_dim * stride - 2*padding + (kernal - 1)\n",
    "            # out_dim = (inp_dim -1) * stride - 2*padding + (kernal -1)\n",
    "# out_dim = strides * (in_dim - 1) + kernel_size - 2 * padding = 2 * (128 - 1) + 4 - 2 * 1 = 256\n",
    "\n",
    "def ct(x, s, k, p, factor):\n",
    "    \n",
    "    x = x\n",
    "    s = s * factor\n",
    "    k = k * factor\n",
    "    p = p * factor\n",
    "    \n",
    "    return s * (x - 1) + k - 2 * p\n",
    "\n",
    "\n",
    "#factor=2\n",
    "\n",
    "output = 2048\n",
    "\n",
    "stride = 2\n",
    "kernal = 4\n",
    "padding = 1\n",
    "\n",
    "for x in [32, 16, 8]:\n",
    "    #y = stride * (x - 1) + kernal - 2 * padding\n",
    "    factor=(output/x)/32\n",
    "    print(\"factor\", factor)\n",
    "    y1 = ct(x, stride, kernal, padding, factor)\n",
    "    y2 = ct(y1, stride, kernal, padding, factor)\n",
    "    y3 = ct(y2, stride, kernal, padding, factor)\n",
    "    y4 = ct(y3, stride, kernal, padding, factor)\n",
    "    y5 = ct(y4, stride, kernal, padding, factor)\n",
    "    print(x, \">>1>>\", y1, \">>2>>\", y2, \">>3>>\", y3, \">>4>>\", y4, \">>5>>\", y5)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NON-FACTOR VERSION\n",
    "\n",
    "from math import sqrt\n",
    "# Conv2D transpose output shape\n",
    "# out_dim = stride * (in_dim - 1) + kernal - 2 * padding\n",
    "            # out_dim = in_dim * stride - 2*padding + (kernal - 1)\n",
    "            # out_dim = (inp_dim -1) * stride - 2*padding + (kernal -1)\n",
    "# out_dim = strides * (in_dim - 1) + kernel_size - 2 * padding = 2 * (128 - 1) + 4 - 2 * 1 = 256\n",
    "\n",
    "def ct(x, s, k, p):\n",
    "    return s * (x - 1) + k - 2 * p\n",
    "\n",
    "\n",
    "#factor=2\n",
    "\n",
    "output = 1024\n",
    "\n",
    "stride = 4 #2\n",
    "kernal = 8  #4\n",
    "padding = 2 #1\n",
    "\n",
    "for x in [64, 128]:\n",
    "    #y = stride * (x - 1) + kernal - 2 * padding\n",
    "\n",
    "    y1 = ct(x, stride, kernal, padding)\n",
    "    y2 = ct(y1, stride, kernal, padding)\n",
    "    y3 = ct(y2, stride, kernal, padding)\n",
    "    y4 = ct(y3, stride, kernal, padding)\n",
    "    print(x, \">>1>>\", y1, \">>2>>\", y2, \">>3>>\", y3, \">>4>>\", y4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two basic types of evaluation metrics are considered:\n",
    "\n",
    "1. The \"continious\" (\"analog\") metric of *mean squared error*.\n",
    "2. Threshold-based (\"dialog\", frequency-based) metrics, assuming a treshold *t* for a classfier *C*, such that for every pixel *x*, if the probaility predicted for *x* (i.e. *p(x)*) is greater than *t*, then *C(x)* = 1, if not, *C(x)* = 0. Represented by a threhold-classification, true positives (TP), false positives (FP), true negatives (TN) and false neagtives (FN) can be calculated and therfore also standard measures of *accuracy*, *recall*, *precision* and *F1*. \n",
    "\n",
    "Both types of metrics (analog and digital) can be measured for the model's performance on *individual* images. However, general measures of the model's performance on the *complete* test set must be considered. For this, two approaches are used:\n",
    "\n",
    "*    A pooled approach: the evaluation metrics are calculated for the concatenation of predictions for every image of the test set in relation to the concatenation of every true label (pixel map of polygon boxes). \n",
    "\n",
    "`Metric([PredictionImage-1 + ... + PredictionImage-n], [TruthImage-1 + ... + TruthImage-n])` (where `+` here stands for concatenation, not addition). \n",
    "*    An averaging approach: taking the mean and standard deviation of a particular metric calculated for individual images \n",
    "\n",
    "`Mean([Metric(image-1), ..., Metric(image-n)])` \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thld_metrics(tp, fp, tn, fn):\n",
    "    \"\"\" Calculates Accuracy, Recall, Precision, and F1 from frequencies of \n",
    "        true postives (tp), false postives (fp), true negatives (tn), and\n",
    "        false neagtives (fn).\n",
    "    \"\"\"\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = (2 * recall * precision) / (recall + precision)\n",
    "    return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "#     print(array)\n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.pooled_mse = \"Not yet defined\"\n",
    "        self.pooled_acc = \"Not yet defined\"\n",
    "        self.pooled_rec = \"Not yet defined\"\n",
    "        self.pooled_prc = \"Not yet defined\"\n",
    "        self.pooled_f1  = \"Not yet defined\"\n",
    "        self.mean_mse = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_acc = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_rec = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_prc = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1  = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.metrics_dict = {\"mse\": [\"Not yet defined\", \"Not yet defined\"], \"accuracy\": [\"Not yet defined\", \"Not yet defined\"], \"recall\": [\"Not yet defined\", \"Not yet defined\"], \"precision\": [\"Not yet defined\", \"Not yet defined\"], \"f1\": [\"Not yet defined\", \"Not yet defined\"]}\n",
    "\n",
    "    def best_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=True)\n",
    "        return m_list[0][0]\n",
    "    \n",
    "    def best_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=True)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def worst_case(self, metric):\n",
    "        \"\"\" Returns the file which has the best performance score with respect \n",
    "            to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=False)\n",
    "        return m_list[0][0]\n",
    "\n",
    "    def worst_cases(self, metric, n):\n",
    "        \"\"\" Returns a list of the N files which has the best performance score \n",
    "            with respect to a metric.\n",
    "        \"\"\"\n",
    "        m_list = self.metrics_dict[metric]\n",
    "        m_list.sort(key=operator.itemgetter(1), reverse=False)\n",
    "        files, values = zip(*m_list)\n",
    "        return list(files[:n])\n",
    "    \n",
    "    def compare(self, other_model):\n",
    "        \"\"\" Compares the evaluation of one model with another.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\" Summarises an evaluation. Returns string.\"\"\"\n",
    "        summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                      f\"Pooled MSE: {self.pooled_mse}\",\n",
    "                      f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                      f\"Pooled Recall: {self.pooled_rec}\",\n",
    "                      f\"Pooled Precision: {self.pooled_prc}\",\n",
    "                      f\"Pooled F1: {self.pooled_f1}\",\n",
    "                      f\"Mean MSE: {self.mean_mse[0]} (std = {self.mean_mse[1]})\",\n",
    "                      f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                      f\"Mean Recall: {self.mean_rec[0]} (std = {self.mean_rec[1]})\",\n",
    "                      f\"Mean Precision: {self.mean_prc[0]} (std = {self.mean_prc[1]})\",\n",
    "                      f\"Mean F1: {self.mean_f1[0]} (std = {self.mean_f1[1]})\"]) \n",
    "        return summary\n",
    "    \n",
    "    def save(self, directory=path_to_save_evaluations):\n",
    "        \"\"\" Writes the summary of an evaluation to a text file (at some diectory).\"\"\"\n",
    "        \n",
    "        summary = self.summary()\n",
    "        with open(directory+self.name, \"w\") as e:\n",
    "            e.write(summary)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\" Prints out the summary of an evaluation.\n",
    "        \"\"\"\n",
    "        summary = self.summary()\n",
    "        print(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch?\n",
    "stride?\n",
    "iterations?\n",
    "window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: performance of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, name, test_data = test_set, threshold = 0.5):\n",
    "    \"\"\" Defines a general pipeline for evaluation by Mean Squared Error (MSE); and optionally\n",
    "        Accuracy, Recall, Precison and F1 (these calculations are time consuming). The evaluator()\n",
    "        function implements the Evaluation class to store and handle informaion from the evaluation. \n",
    "        For each metric, there is a value calculated for the test data as a whole (\"pooled\") and an \n",
    "        average value calculated over the set of values calculated for each image individually. \n",
    "        Besides an instance of the Evaluation class, evaluator() returns a mapping between files and \n",
    "        the predicted outcome for that file.\n",
    "    \"\"\"\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    evaluation = Evaluation(name)\n",
    "    \n",
    "    prediction_pooled = []\n",
    "    truth_pooled = [] \n",
    "    thld_frequencies_pooled = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "    thld_metrics_calc = {\"mse\": [], \"accuracy\": [], \"recall\": [], \"precision\": [], \"f1\": []}\n",
    "\n",
    "    i=1 # in order to print out progress\n",
    "    for instance in test_data:\n",
    "        \n",
    "        prediction = torch.flatten(model(instance[\"img_vector\"])) # due to design of model\n",
    "        truth = torch.flatten(instance[\"label\"]).int()\n",
    "        file = instance[\"file\"]\n",
    "        \n",
    "        prediction_pooled.append( (file, prediction) )\n",
    "        truth_pooled.append(truth)        \n",
    "        \n",
    "        mse = F.mse_loss(prediction, truth)\n",
    "        thld_metrics_calc[\"mse\"].append( (file, mse.item()) )\n",
    "        \n",
    "        \n",
    "        #######\n",
    "        #roundof = (prediction >= 0.1).int()\n",
    "        #print(\"sum of roundof, file:\", file, torch.sum(roundof))\n",
    "        #######\n",
    "        \n",
    "        # IF-block for calculating accuracy, recall, precision, f1, which is very\n",
    "        # time consuming, due to identifications of TPs, FPs, TNs, and FNs for \n",
    "        # large matrices.\n",
    "        if threshold != None:\n",
    "            roundof = (prediction >= threshold).int()\n",
    "            #print(\"sum of roundof\", torch.sum(roundof))\n",
    "            tp = sum(roundof * truth)\n",
    "            fp = sum(roundof * (~truth.bool()))\n",
    "            tn = sum((~roundof.bool()) * (~truth.bool()))\n",
    "            fn = sum((~roundof.bool()) * truth)\n",
    "\n",
    "            accuracy, recall, precision, f1 = thld_metrics(tp, fp, tn, fn)\n",
    "\n",
    "            for key, value in zip([\"tp\", \"fp\", \"tn\", \"fn\"], [tp, fp, tn, fn]):\n",
    "                thld_frequencies_pooled[key]+=value\n",
    "\n",
    "            for key, value in zip([\"accuracy\", \"recall\", \"precision\", \"f1\"], \n",
    "                                  [accuracy.item(), recall.item(), precision.item(), f1.item()]):\n",
    "                thld_metrics_calc[key].append( (file, value) )\n",
    "        \n",
    "        print(\"({}%)\".format(round((i/len(test_data)*100), 1)), end=\"\\r\")\n",
    "        i+=1\n",
    "        \n",
    "        #print(\"TP\", tp, \"FP\", fp, \"TN\", tn, \"FN\", fn)\n",
    "        #break\n",
    "    \n",
    "    file, predictions = zip(*prediction_pooled)\n",
    "    evaluation.pooled_mse = F.mse_loss(torch.flatten(torch.stack(list(predictions))), \n",
    "                                       torch.flatten(torch.stack(truth_pooled))).item()\n",
    "    \n",
    "    if threshold != None:\n",
    "        pooled_accuracy, pooled_recall, pooled_precision, pooled_f1 = thld_metrics(\n",
    "            thld_frequencies_pooled[\"tp\"], \n",
    "            thld_frequencies_pooled[\"fp\"], \n",
    "            thld_frequencies_pooled[\"tn\"], \n",
    "            thld_frequencies_pooled[\"fn\"])\n",
    "        \n",
    "        evaluation.pooled_acc = pooled_accuracy\n",
    "        evaluation.pooled_rec = pooled_recall\n",
    "        evaluation.pooled_prc = pooled_precision\n",
    "        evaluation.pooled_f1  = pooled_f1\n",
    "    \n",
    "    # The code below is a bit nested. What it does in plain English is:\n",
    "    # go to the dictionary where we keep all the performance scores with respect to \n",
    "    # each file. Every key (i.e. metric) of that dict maps to a list of tupples of \n",
    "    # file and value of the metric. Here, we \"unzip\" that list of tupples and calculate the \n",
    "    # mean (and standard deviation) for the values and use that mean (and std) to define \n",
    "    # the respective attributes of the Evaluation class instance.\n",
    "    \n",
    "    evaluation.mean_mse = mean(list(zip(*thld_metrics_calc[\"mse\"]))[1])\n",
    "    \n",
    "    if threshold != None:\n",
    "        evaluation.mean_acc = mean(list(zip(*thld_metrics_calc[\"accuracy\"]))[1]) \n",
    "        evaluation.mean_rec = mean(list(zip(*thld_metrics_calc[\"recall\"]))[1]) \n",
    "        evaluation.mean_prc = mean(list(zip(*thld_metrics_calc[\"precision\"]))[1]) \n",
    "        evaluation.mean_f1  = mean(list(zip(*thld_metrics_calc[\"f1\"]))[1]) \n",
    "    \n",
    "    evaluation.metrics_dict = thld_metrics_calc\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    passed_time = t2 - t1\n",
    "    print(\"Done! ({} m., {} s.)\".format(int(passed_time/60), int(passed_time%60)))\n",
    "    \n",
    "    return evaluation, dict(prediction_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to revisit the performance of a trained and saved model with its evaluation data\n",
    "eval_files = [instance[\"file\"] for instance in test_set]\n",
    "with open(path_to_save_evaluations + f\"{cup_name}.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(eval_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_cup, file2pred_map = evaluator(my_cup_model, cup_name, threshold = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_cup.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_best_file = evaluation_cup.best_case(\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_worst_file = evaluation_cup.worst_case(\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_cup.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diabolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to revisit the performance of a trained and saved model with its evaluation data\n",
    "eval_files = [instance[\"file\"] for instance in test_set]\n",
    "with open(path_to_save_evaluations + f\"{diab_name}.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(eval_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_diabolo, file2pred_map = evaluator(my_diabolo_model, diab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation_diabolo.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_best_file = evaluation_diabolo.best_case(\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_worst_file = evaluation_diabolo.worst_case(\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_diabolo.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(file, \n",
    "              file2prediction, \n",
    "              file2annotation, # the mapping of files with their annotations was defined above\n",
    "              height = 2048, \n",
    "              width = 2048, \n",
    "              rescale = rescale_output_to,\n",
    "              img_dir = images_dir, \n",
    "              colorscheme = \"Reds\", \n",
    "              alpha_value = 0.3):\n",
    "    \"\"\" Visualizes the performance of a model on a particular image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # FIRST, collect elements of figure\n",
    "    img = Image.open(img_dir+file)\n",
    "    if rescale != None:\n",
    "        img = img.resize((rescale, rescale))\n",
    "    \n",
    "    if rescale != None:\n",
    "        rescale_by = rescale / height # must come first\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "\n",
    "    polygons = []\n",
    "    for block in file2annotation[file]:  \n",
    "        for character in block:\n",
    "            if character[\"is_chinese\"] == True:\n",
    "                polygons.append(character[\"polygon\"])\n",
    "    \n",
    "    if rescale != None:\n",
    "        polygons = [[[point * rescale_by for point in points] for points in set_of_points] for set_of_points in polygons]\n",
    "    \n",
    "    cpu_tensor = file2prediction[file].cpu().detach().numpy()\n",
    "    heat = cpu_tensor.reshape(height, width)\n",
    "    #heat = file2prediction[file].reshape(height, width).detach().numpy()\n",
    "    \n",
    "    # SECOND, arrange and plot elements in 2 x 2 subplots\n",
    "    \n",
    "    %matplotlib inline\n",
    "    \n",
    "    figure = plt.figure(figsize=(16, 16))\n",
    "    \n",
    "    #Up-Left\n",
    "    axUL = figure.add_subplot(221)\n",
    "    axUL.set_title(\"True polygon box on image (Target)\")\n",
    "    axUL.imshow(img)\n",
    "    for polygon in polygons:\n",
    "        polly = Polygon(polygon, fill = False, color = (0, 1, 0)) # Recall that Polygon is a method of matplotlib.patches\n",
    "        axUL.add_patch(polly)\n",
    "\n",
    "    #Up-Right\n",
    "    axUR = figure.add_subplot(222)\n",
    "    axUR.set_title(\"Predicted heatmap on image (Result)\")\n",
    "    axUR.imshow(img)\n",
    "    axUR.imshow(heat, cmap = colorscheme, alpha = alpha_value)\n",
    "\n",
    "    #Down-Left\n",
    "    axDL = figure.add_subplot(223)\n",
    "    axDL.set_title(\"Image, Target and Result\")\n",
    "    axDL.imshow(img)\n",
    "    axDL.imshow(heat, cmap = colorscheme, alpha = alpha_value)\n",
    "    for polygon in polygons:\n",
    "        polly = Polygon(polygon, fill = False, color = (0, 1, 0))\n",
    "        axDL.add_patch(polly)\n",
    "\n",
    "    #Down-Right\n",
    "    axDR = figure.add_subplot(224)\n",
    "    axDR.set_title(\"True box on predicted heatmap (Abstraction)\")\n",
    "    axDR.imshow(heat, cmap = colorscheme, alpha = alpha_value)\n",
    "    for polygon in polygons:\n",
    "        polly = Polygon(polygon, fill = False, color = (0, 1, 0))\n",
    "        axDR.add_patch(polly)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(\"1002570.jpg\", file2pred_map, mapping, colorscheme = \"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize(my_best_file, file2pred_map, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(my_worst_file, file2pred_map, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

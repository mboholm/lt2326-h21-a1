{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Chinese character \"detection\"\n",
    "LT2326, Autumn 2021\n",
    "\n",
    "Name: Max Boholm (gusbohom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook cotains the code for *Assignment 1: Chinese character \"detection\"* for the course *Machine learning for statistical NLP: Advanced* (course code LT2326), Autumn 2021. The notebook is organized into the folowing parts:\n",
    "\n",
    "*    Meta variables (the term *hyperparameter* is here reserved for decisions on the models), which define ... the loacation (path) of the data, ... \n",
    "*    Data preparation\n",
    "*    Definition and training of two models\n",
    "*    Testing and evaluation\n",
    "*    ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THINK AGAIN ABOUT WHAT THE PROBLEM IS**\n",
    "How do you go from the \"topographic\" representation back to coordinates?\n",
    "Consider using coordinates as targets, how do you calculate the loss? (Perhaps not that hard... one output for every dimension of coordinates... but then: ) How to handle variable length of boxes in image?\n",
    "\n",
    "???\n",
    "predict coordinates\n",
    "for a 2048 x 2048 vector translate those coordinates to 0s and 1s\n",
    "compute loss for that vector\n",
    "\n",
    "\n",
    "*    Rescaling - how stupid is my approach? WORKS FOR DEVELOPMENT\n",
    "*    How to build (convolutional) models?\n",
    "    *    Does I have to consider padding? (If so, perhaps best to use a `torch` dataloader) I THINK THE CONVOLUTIONAL LAYERS HELPS ME WITH THAT\n",
    "*    How to visualize?\n",
    "    *    Reverse flattening\n",
    "    *    Heatmap simple\n",
    "    *    Heatmap-layer\n",
    "*    Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.path as mplpath\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"../../scratch/lt2326-h21/a1/\"\n",
    "path = \"../develop_util/\"\n",
    "images_dir = path + \"images/\"      # the directory of images to be data\n",
    "meta_ctw   = path + \"info.json\"   # the (path to) the general meta file of CTW \n",
    "meta_train = path + \"train.jsonl\" # the (path to) the file containing the annotations of CTW training data\n",
    "\n",
    "train_proportion = 0.7 # the proportion of training data; proportion of test data will be the complement of this number\n",
    "\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "path_to_save_models = \"../models/\"\n",
    "\n",
    "# For development purposes\n",
    "restriction = None # set to an integer, if subsample of the data is to be used in e.g. the development phase\n",
    "rescale_input_to = 50 # ...\n",
    "rescale_output_to = 128 # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Decision:* batching is kept as list until feeding it to model (`flat_batch`). The list makes the evaluation part simpler as we do not have to bend our minds around multi-dimensional tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_train(file_dir = images_dir, meta_file = meta_ctw):\n",
    "    \"\"\"Tests whether image files in a directory is part of the training set of the CTW dataset, \n",
    "       as defined by the json metafile for CTW. Returns a list of filenames such that they are both\n",
    "       in the specified directory and the CTW training set.\n",
    "    \"\"\"\n",
    "    meta = json.load(open(meta_file,\"r\"))\n",
    "    train_files_CTW = [entry[\"file_name\"] for entry in meta[\"train\"]]\n",
    "    files_to_keep = []\n",
    "    potential_files=[file.split(\"/\")[-1] for file in glob.glob(file_dir+\"*.jpg\")]\n",
    "    \n",
    "    for file in potential_files:\n",
    "        if file in train_files_CTW:\n",
    "            files_to_keep.append(file)\n",
    "            \n",
    "    return files_to_keep\n",
    "\n",
    "def CTW_mapper(files, meta = meta_train):\n",
    "    \"\"\" Identifies annotations for files from the training set of the CTW dataset. \n",
    "        Returns pyhoton dictionary that maps filenames (keys) with annotations (values), \n",
    "        which like in the original format is a list of lists of json elements / python dictinaries. \n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    with open(meta, \"r\") as f:\n",
    "        annotations_data = [json.loads(line) for line in f.readlines()]\n",
    "        for file in files:\n",
    "            for annotation in annotations_data:\n",
    "                if annotation[\"image_id\"] == file[:-4]:\n",
    "                    mapping[file] = annotation[\"annotations\"]\n",
    "                    break\n",
    "    \n",
    "    return mapping\n",
    "                    \n",
    "def img2array(file, directory = images_dir, rescale = rescale_input_to):\n",
    "    \"\"\" Takes a filename of an image in a directory and returns an numpy array \n",
    "        corresponding to the image.\n",
    "    \"\"\"\n",
    "    img = Image.open(directory+file)\n",
    "    \n",
    "    if rescale != None:\n",
    "        img = img.resize((rescale, rescale))\n",
    "    \n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    return img_np\n",
    "\n",
    "def bbox2array(file, mapping, height  = 2048, width   = 2048, rescale = rescale_output_to): \n",
    "    \"\"\" Builds a matrix of 0s and 1s representing the bbox as \n",
    "        defined by the coordinates of in the CTW annotations.\n",
    "    \"\"\"\n",
    "    \n",
    "    if rescale != None:\n",
    "        rescale_by = rescale / height # must come first\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "    \n",
    "    char_matrix = np.zeros((height, width))\n",
    "    \n",
    "    char_areas = []\n",
    "    for block in mapping[file]:\n",
    "        for character in block:\n",
    "            if character[\"is_chinese\"] == True:\n",
    "                xmin = int(character[\"adjusted_bbox\"][0])\n",
    "                ymin = int(character[\"adjusted_bbox\"][1])\n",
    "                w    = int(character[\"adjusted_bbox\"][2])\n",
    "                h    = int(character[\"adjusted_bbox\"][3])\n",
    "                \n",
    "                if rescale != None:\n",
    "                    xmin = int(xmin * rescale_by)\n",
    "                    ymin = int(ymin * rescale_by)\n",
    "                    w    = int(w    * rescale_by)\n",
    "                    h    = int(h    * rescale_by)\n",
    "                \n",
    "                char_areas.append((xmin, ymin, w, h))\n",
    "    \n",
    "    for xmin, ymin, w, h in char_areas:\n",
    "        r1 = height - ymin - h\n",
    "        r2 = r1 + h\n",
    "        c1 = width - xmin\n",
    "        c2 = c1 + w\n",
    "        \n",
    "        char_matrix[r1:r2 , c1:c2] = 1\n",
    "        \n",
    "    return char_matrix\n",
    "\n",
    "def polygon2array(file, mapping, height = 2048, width = 2048, rescale = rescale_output_to):\n",
    "    \"\"\" Builds a matrix of 0s and 1s representing the character polygons as \n",
    "        defined by the coordinates of in the CTW annotations. \n",
    "    \"\"\"\n",
    "    \n",
    "    polygons = []\n",
    "    for block in mapping[file]: # mapping maps files with their annotations\n",
    "        for character in block:\n",
    "            if character[\"is_chinese\"] == True:\n",
    "                polygons.append(character[\"polygon\"])\n",
    "    \n",
    "    if rescale != None:\n",
    "        rescale_by = rescale / height # must come first\n",
    "        height = rescale\n",
    "        width = rescale\n",
    "        polygons = [[[point * rescale_by for point in points] for points in set_of_points] for set_of_points in polygons]\n",
    "    \n",
    "    every_point = np.array([[[h,w] for h in list(range(height))] for w in list(range(width))]).reshape(height*width, 2)\n",
    "    zeros_to_update = np.zeros(height * width)\n",
    "    \n",
    "    for polygon in polygons:\n",
    "        path = mplpath.Path(np.array(polygon))\n",
    "        hits = np.asarray(path.contains_points(every_point), int)\n",
    "        zeros_to_update += hits\n",
    "    \n",
    "    matrix = zeros_to_update.reshape(height, width)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def data_builder(files, directory, mapping, restriction = restriction):\n",
    "    \"\"\" Compiles the dataset for use. Returns a list of dictionaties, such that each\n",
    "        element of the list contains:\n",
    "        -  the filename; key: \"file\"\n",
    "        -  a vectorized instance of the training data; key: \"img_vector\"\n",
    "        -  a vectorized instance of the labels, or targets (a vector of 0s and 1s indicating \n",
    "           boxes of characters in images); key: \"label\"\n",
    "        \n",
    "        Note: (1) the format of instances (training input and targets) are numpy arrays; and\n",
    "        (2) the instances have \"matrix shape\". For these reasons, the output of the data_builder()\n",
    "        requires further processing for it to be ready for pytorch processing. The functions \n",
    "        standardizer() and numpy2torch() is does required further down the pipline of data\n",
    "        preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for file in files: \n",
    "        instance = {}\n",
    "        instance[\"file\"] = file\n",
    "        instance[\"img_vector\"] = img2array(file)\n",
    "        #instance[\"label\"] = bbox2array(file, mapping)\n",
    "        instance[\"label\"] = polygon2array(file, mapping)\n",
    "        data.append(instance) \n",
    "    \n",
    "    if restriction != None:\n",
    "        random.shuffle(data)\n",
    "        data = data[:restriction]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def standardizer(dataset, scaler = StandardScaler()):\n",
    "    \"\"\" Standardizes the image vectors of a dataset to z-scores using StandardScaler() \n",
    "        from the library sklearn.preprocessing. \n",
    "    \"\"\"\n",
    "    std_data = []\n",
    "    #N = len(dataset)\n",
    "    example = dataset[0][\"img_vector\"]\n",
    "    x, y, z = example.shape\n",
    "    n_features = example.size # ... or x * y * z\n",
    "    \n",
    "    for instance in dataset:\n",
    "        std_data.append(instance[\"img_vector\"].reshape(n_features))\n",
    "    \n",
    "    scaled_data = scaler.fit_transform(std_data)\n",
    "    \n",
    "    for i, scaled_ins in enumerate(scaled_data):\n",
    "        dataset[i][\"img_vector\"] = scaled_ins.reshape(x, y, z)\n",
    "\n",
    "def numpy2torch(dataset, device = device, permute = True):\n",
    "    \"\"\" For a dataset, transforms its numpy arrays to torch tensors. If permute = True,\n",
    "        image vectors are permuted such that ... \n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"Shape of vector before: \", dataset[0][\"img_vector\"].shape)\n",
    "    \n",
    "    for instance in dataset:\n",
    "        if permute == True: # ... hmmm \n",
    "            instance[\"img_vector\"] = torch.Tensor(instance[\"img_vector\"], device = device).permute(2,0,1)\n",
    "        else:\n",
    "            instance[\"img_vector\"] = torch.Tensor(instance[\"img_vector\"], device = device)\n",
    "        instance[\"label\"] = torch.Tensor(instance[\"label\"], device = device)\n",
    "    \n",
    "    #print(\"Shape of vector after: \", dataset[0][\"img_vector\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling functions: creating the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = only_train()\n",
    "mapping = CTW_mapper(files)\n",
    "my_data = data_builder(files, images_dir, mapping)\n",
    "standardizer(my_data)\n",
    "numpy2torch(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as .jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"dataset.jsonl\", \"r\") as d:\n",
    "#    my_data = [json.load(instance) for instance in d] # this will not handle np stuff I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(data       = my_data, \n",
    "          train_prop = train_proportion, \n",
    "          val_prop   = None):\n",
    "    \"\"\" Splits a dataset into training data, testing data and, if selected,\n",
    "        validation data. Note that the proportions of training data, test data\n",
    "        and validation data (optional) must not exceed 100%. \n",
    "    \"\"\"\n",
    "    \n",
    "    if val_prop != None:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        val_to_idx   = int(len(data) * val_prop) + train_to_idx\n",
    "        train = data[:train_to_idx]\n",
    "        val   = data[train_to_idx:val_to_idx]\n",
    "        test  = data[val_to_idx:]\n",
    "        return train, val, test\n",
    "    else:\n",
    "        train_to_idx = int(len(data) * train_prop)\n",
    "        train = data[:train_to_idx]\n",
    "        test  = data[train_to_idx:]\n",
    "        return train, test\n",
    "\n",
    "train_set, test_set = split() #val?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a dataloader\n",
    "In training, the function will be called with the `train_set` as argument in every iteration (epoch) yielding  randomized and batched traing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we need padding?! no part of convolutional layer\n",
    "\n",
    "def dataloader(data, batch_size):\n",
    "    \"\"\" Takes a (proportion of) a dataset and returns a randomized iterator \n",
    "        of the data organized into batches as defined by batch_size.\n",
    "        \n",
    "        Note: the dataloader preserves the \"matrix shape\" of trainingdata and \n",
    "        targets. Since pytorch neural networks require \"flat\" shapes of data\n",
    "        the function flat_batch() is used to let data flow trough training\n",
    "        in the desired format. \n",
    "    \"\"\"\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    for group in [data[i : i+batch_size] for i in range(0, len(data), batch_size)]:\n",
    "        files = []\n",
    "        img_vecs = []\n",
    "        labels = []\n",
    "\n",
    "        for instance in group:\n",
    "            files.append(instance[\"file\"])\n",
    "            img_vecs.append(instance[\"img_vector\"])\n",
    "            labels.append(instance[\"label\"])\n",
    "\n",
    "        batch = {\"file\":files, \n",
    "                 \"img_vector\":img_vecs, \n",
    "                 \"label\":labels}\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '0000174.jpg',\n",
       " 'img_vector': tensor([[[ 1.5574,  1.5627,  1.5741,  ...,  0.5990, -0.1580,  0.6819],\n",
       "          [ 1.5664,  1.5732,  1.5761,  ..., -0.0143,  0.3681, -0.0704],\n",
       "          [ 1.5640,  1.5757,  1.5463,  ...,  0.7814,  0.5509,  1.1346],\n",
       "          ...,\n",
       "          [-1.3626, -1.3501, -1.3124,  ...,  1.7152,  1.7382,  1.4885],\n",
       "          [-1.4054, -1.3861, -1.2840,  ...,  1.6274,  1.8201,  1.7719],\n",
       "          [-1.2637, -1.2589, -1.2508,  ...,  1.4272,  1.6324,  1.7605]],\n",
       " \n",
       "         [[ 1.4955,  1.5454,  1.5554,  ...,  0.2724, -0.6234,  0.3421],\n",
       "          [ 1.5248,  1.5440,  1.5131,  ..., -0.5830,  0.0598, -0.4879],\n",
       "          [ 1.5086,  1.5072,  1.5112,  ...,  0.3654,  0.1018,  0.9168],\n",
       "          ...,\n",
       "          [-1.4167, -1.3837, -1.3778,  ...,  1.6411,  1.6386,  1.3897],\n",
       "          [-1.4553, -1.4766, -1.3444,  ...,  1.5661,  1.7879,  1.7393],\n",
       "          [-1.3110, -1.2970, -1.2832,  ...,  1.3103,  1.5133,  1.7198]],\n",
       " \n",
       "         [[ 1.1902,  1.2638,  1.4043,  ..., -0.4131, -1.4313, -0.3194],\n",
       "          [ 1.2074,  1.2907,  1.3256,  ..., -1.4978, -0.6832, -1.2307],\n",
       "          [ 1.1967,  1.2880,  1.3443,  ..., -0.3711, -0.8591,  0.4316],\n",
       "          ...,\n",
       "          [-1.4082, -1.3540, -1.3821,  ...,  1.5079,  1.4630,  1.2392],\n",
       "          [-1.4528, -1.4608, -1.3422,  ...,  1.4454,  1.7012,  1.6181],\n",
       "          [-1.2751, -1.2805, -1.2290,  ...,  1.2034,  1.3760,  1.6343]]]),\n",
       " 'label': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General traing procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_batch(batch):\n",
    "    \"\"\"Takes a python list of length B of more-than-one dimensional tensors (N, M, ...) and \n",
    "    returns a tensor of shape: (B, M*N*...)...\"\"\"\n",
    "    \n",
    "    return torch.stack([torch.flatten(instance) for instance in batch])\n",
    "\n",
    "# do we need device assignation here?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, # Must be an instance of a model!\n",
    "            name_of_model,\n",
    "            learning_rate,\n",
    "            epochs,\n",
    "            batch_size,\n",
    "            train_data = train_set,\n",
    "            val_data = None,\n",
    "            save_model = False,\n",
    "            path_for_saving_model = path_to_save_models,\n",
    "            my_optimizer = optim.Adam,\n",
    "            my_loss_function = nn.BCELoss()):\n",
    "    \"\"\" Specifices a general training procedure for a model. \n",
    "        Note: trainer() requires an instantiated model as model argument. \n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = my_optimizer(model.parameters(), lr=learning_rate)    \n",
    "    \n",
    "    #model = my_model\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    loss_function = my_loss_function\n",
    "    \n",
    "    #total_loss = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        iterator = dataloader(train_set, batch_size)\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            output = model(batch[\"img_vector\"])\n",
    "            targets = flat_batch(batch[\"label\"])\n",
    "            \n",
    "            loss = loss_function(output, targets)\n",
    "            \n",
    "            #total_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            #print(\"Epoch: \", epoch+1, \"Batch: \", i, \"Total loss: \", total_loss/(i+1), end='\\r')\n",
    "            loss.backward() # compute gradients\n",
    "            optimizer.step() # update parameters\n",
    "            optimizer.zero_grad # reset gradients\n",
    "            \n",
    "        #print()\n",
    "        print(f\"Epoch: {epoch+1} (out of {epochs}); total loss: {epoch_loss}.\")\n",
    "\n",
    "            \n",
    "        if val_data != None:\n",
    "            model.eval()\n",
    "            \n",
    "            # DO SOMETHING\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "    if save_model == True:\n",
    "        torch.save(model, path_for_saving_model+name_of_model+\".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple model, basically passing data from input to output. It is **used for development reasons only**. I have kept it here as it might become useful for future development, but *it can be ignored for examination of Assignment 1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model defintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, inp, hidden, outp):  \n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(inp, hidden)\n",
    "        self.classifier = nn.Linear(hidden, outp)\n",
    "  \n",
    "    def forward(self, batch): \n",
    "        \n",
    "        flat = flat_batch(batch)\n",
    "        compression = F.relu(self.layer1(flat))\n",
    "        output = torch.sigmoid(self.classifier(compression))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = my_data[0][\"img_vector\"].shape\n",
    "input_size = shape_of_input[0] * shape_of_input[1] * shape_of_input[2]\n",
    "shape_of_output = my_data[0][\"label\"].shape\n",
    "output_size = shape_of_output[0] * shape_of_output[1]\n",
    "\n",
    "#print(shape_of_input)\n",
    "#print(shape_of_output)\n",
    "\n",
    "my_simple_model = SimpleModel(inp=input_size, hidden=100, outp=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SimpleModel(\n",
       "  (layer1): Linear(in_features=7500, out_features=100, bias=True)\n",
       "  (classifier): Linear(in_features=100, out_features=16384, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_simple_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 2); total loss: 0.695411205291748.\n",
      "Epoch: 2 (out of 2); total loss: 0.7315812110900879.\n"
     ]
    }
   ],
   "source": [
    "trainer(my_simple_model, # Must be an instance of a model!\n",
    "        \"simplemodel.pt\",\n",
    "        learning_rate=0.005,\n",
    "        epochs=2,\n",
    "        batch_size=2,\n",
    "        save_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Diabolo Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiaboloModel(nn.Module):\n",
    "    def __init__(self, inp, outp):   \n",
    "        super(DiaboloModel, self).__init__()\n",
    "        \n",
    "        self.input = inp\n",
    "        self.output = outp\n",
    "        \n",
    "        self.compression1 = int(self.input / 2)\n",
    "        self.compression2 = int(self.input / 4)\n",
    "        self.compression3 = int(self.input / 8)\n",
    "        self.compression4 = int(self.input / 16)\n",
    "        \n",
    "        self.dilation1 = int(self.output / 8)\n",
    "        self.dilation2 = int(self.output / 4)\n",
    "        self.dilation3 = int(self.output / 2)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input, self.compression1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.compression1, self.compression2),\n",
    "            nn.ReLU(),           \n",
    "            nn.Linear(self.compression2, self.compression3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.compression3, self.compression4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.compression4, self.dilation1),\n",
    "            #nn.Dropout(0.05),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dilation1, self.dilation2),\n",
    "            nn.ReLU(),            \n",
    "            nn.Linear(self.dilation2, self.dilation3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.dilation3, self.output),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    " \n",
    "    def forward(self, batch): \n",
    "        \n",
    "        flat = flat_batch(batch)\n",
    "        encoded = self.encoder(flat)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_of_input = my_data[0][\"img_vector\"].shape\n",
    "input_size = shape_of_input[0] * shape_of_input[1] * shape_of_input[2]\n",
    "shape_of_output = my_data[0][\"label\"].shape\n",
    "output_size = shape_of_output[0] * shape_of_output[1]\n",
    "\n",
    "#print(shape_of_input)\n",
    "#print(shape_of_output)\n",
    "\n",
    "my_diabolo_model = DiaboloModel(inp=input_size, outp=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DiaboloModel(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=7500, out_features=3750, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3750, out_features=1875, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=1875, out_features=937, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=937, out_features=468, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=468, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=8192, out_features=16384, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_diabolo_model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (out of 2); total loss: 0.6931681036949158.\n",
      "Epoch: 2 (out of 2); total loss: 1.9287109375.\n"
     ]
    }
   ],
   "source": [
    "nepochs = 2\n",
    "batchsz = 2\n",
    "name = \"Diab_{}e{}b\".format(nepochs, batchsz)\n",
    "\n",
    "trainer(my_diabolo_model, # Must be an instance of a model!\n",
    "        name,\n",
    "        learning_rate=0.005,\n",
    "        epochs=nepochs,\n",
    "        batch_size=batchsz,\n",
    "        save_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.1: Convolutional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, inp_height, inp_width, outp):   \n",
    "        super(ConvModel, self).__init__()\n",
    "        \n",
    "        self.height = inp_height\n",
    "        self.width = inp_width\n",
    "        self.getting_out = self.height * self.width * 3 \n",
    "        # 3 for channels\n",
    "        # ... plus operation to acount for pooling stuff\n",
    "        \n",
    "        self.output = outp\n",
    "        self.dilation = self.output / 2\n",
    "\n",
    "        \n",
    "        self.convolution = nn.Sequential(\n",
    "            Conv2d(3, 3, 3, stride = 1, padding=True), # 3 channels in (RGB), 3 out, window of 3x3\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2), # 2x2 window with stride of 2\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(self.getting_out, self.dialation),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.dialation, self.output),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    " \n",
    "    def forward(self, batch): \n",
    "        \n",
    "        batch = torch.stack(batch) # device?\n",
    "        features = self.convolution(batch)\n",
    "        output = self.out(features)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2.2: Convolutional Model with Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: testing and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two basic types of evaluation metrics are considered:\n",
    "\n",
    "1. The \"continious\" (\"analog\") metric of *mean squared error*.\n",
    "2. Threshold-based (\"dialog\", frequency-based) metrics, assuming a treshold *t* for a classfier *C*, such that for every pixel *x*, if the probaility predicted for *x* (i.e. *p(x)*) is greater than *t*, then *C(x)* = 1, if not, *C(x)* = 0. Represented by a threhold-classification, true positives (TP), false positives (FP), true negatives (TN) and false neagtives (FN) can be calculated and therfore also standard measures of *accuracy*, *recall*, *precision* and *F1*. \n",
    "\n",
    "Both types of metrics (analog and digital) can be measured for the model's performance on *individual* images. However, general measures of the model's performance on the *complete* test set must be considered. For this, two approaches are used:\n",
    "\n",
    "*    A pooled approach: the evaluation metrics are calculated for the concatenation of predictions for every image of the test set in relation to the concatenation of every true label (pixel map of polygon boxes). \n",
    "\n",
    "`Metric([PredictionImage-1 + ... + PredictionImage-n], [TruthImage-1 + ... + TruthImage-n])` (where `+` here stands for concatenation, not addition). \n",
    "*    An averaging approach: taking the mean and standard deviation of a particular metric calculated for individual images \n",
    "\n",
    "`Mean([Metric(image-1), ..., Metric(image-n)])` \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def th_metrics(tp, fp, tn, fn):\n",
    "    \"\"\" Calculates Accuracy, Recall, Precision, and F1 from frequencies of \n",
    "        true postives (tp), false postives (fp), true negatives (tn), and\n",
    "        false neagtives (fn).\n",
    "    \"\"\"\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    f1 = (2 * recall * precision) / (recall + precision)\n",
    "    return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(array):\n",
    "    \"\"\" Calculates the mean and standard deviation of an aray of numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = np.mean(array)\n",
    "    std  = np.std(array)\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\" For storing and handling information from the evaluation of models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.pooled_mse = \"Not yet defined\"\n",
    "        self.pooled_acc = \"Not yet defined\"\n",
    "        self.pooled_rec = \"Not yet defined\"\n",
    "        self.pooled_prc = \"Not yet defined\"\n",
    "        self.pooled_f1  = \"Not yet defined\"\n",
    "        self.mean_mse = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_acc = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_rec = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_prc = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.mean_f1  = (\"Not yet defined\", \"Not yet defined\")\n",
    "        self.summary  = \"\\n\".join([f\"Model {self.name} performs as follows:\", \n",
    "                                   f\"Pooled MSE: {self.pooled_mse}\",\n",
    "                                   f\"Pooled Accuracy: {self.pooled_acc}\",\n",
    "                                   f\"Pooled Recall: {self.pooled_rec}\",\n",
    "                                   f\"Pooled Precision: {self.pooled_prc}\",\n",
    "                                   f\"Pooled F1: {self.pooled_f1}\",\n",
    "                                   f\"Mean MSE: {self.mean_mse[0]} (std = {self.mean_mse[1]})\",\n",
    "                                   f\"Mean Accuracy: {self.mean_acc[0]} (std = {self.mean_acc[1]})\",\n",
    "                                   f\"Mean Recall: {self.mean_rec[0]} (std = {self.mean_rec[1]})\",\n",
    "                                   f\"Mean Precision: {self.mean_prc[0]} (std = {self.mean_prc[1]})\",\n",
    "                                   f\"Mean F1: {self.mean_f1[0]} (std = {self.mean_f1[1]})\"])\n",
    "        \n",
    "\n",
    "    \n",
    "    def compare(self, evaluation_model1, evaluation_model2):\n",
    "        pass\n",
    "    \n",
    "    def save(self, path, filename):\n",
    "        \n",
    "        with open(path+filename, \"w\") as e:\n",
    "            e.write(self.summary)\n",
    "    \n",
    "    def summary(self):\n",
    "        txt = self.summary\n",
    "        print(txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = Evaluation(\"mamma_mu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"mamma\\nmu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing: setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch?\n",
    "stride?\n",
    "iterations?\n",
    "window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: performance of best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    Pooled metrics (concatenate prediction-1, ..., prediction-n (P) and label-1, ..., label-n (L) and calculate metric(P, L).\n",
    "*    Means (with std)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, threshold = 0.5):\n",
    "    \"\"\" Defines a pipeline for evaluation by evaluation metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # evaluation = Evaluation(name)\n",
    "    \n",
    "    \n",
    "    # mse_pooled = ... 1. make a list of inp_vecs / labels; 2. make a torch tensor; 3. feed to model; 4. \"evaluate\"\n",
    "    # truth_pooled = ... \n",
    "    mse_calculated = []\n",
    "    \n",
    "    th_frequencies_pooled = {\"tp\": 0, \"fp\": 0, \"tn\": 0, \"fn\": 0}\n",
    "    th_metrics_calc = {\"accuracy\": [], \"recall\": [], \"precision\": [], \"f1\": []}\n",
    "\n",
    "    for instance in test_set:\n",
    "        \n",
    "        prediction = model(torch.flatten(instance[\"img_vector\"]))\n",
    "        truth = torch.flatten(instance[\"label\"]).int()\n",
    "        #print(truth)\n",
    "\n",
    "        mse = F.mse_loss(prediction, truth)\n",
    "        mse_calculated.append(mse.item())\n",
    "        \n",
    "        roundof = (prediction >= threshold).int()\n",
    "        \n",
    "        tp = sum(roundof * truth)\n",
    "        #print(\"TP: \", tp)\n",
    "        fp = sum(roundof * (~truth.bool()).float())\n",
    "        #print(\"FP: \", fp)\n",
    "        tn = sum((~roundof.bool()).float() * (~truth.bool()).float())\n",
    "        #print(\"TN: \", tn)\n",
    "        fn = sum((~roundof.bool()).float() * truth)\n",
    "        #print(\"FN: \", fn)\n",
    "        \n",
    "        accuracy, recall, precision, f1 = th_metrics(tp, fp, tn, fn)\n",
    "        \n",
    "        for key, value in zip([\"tp\", \"fp\", \"tn\", \"fn\"], [tp.item(), fp.item(), tn.item(), fn.item()]):\n",
    "            th_frequencies_pooled[key]+=value\n",
    "        \n",
    "        for key, value in zip([\"accuracy\", \"recall\", \"precision\", \"f1\"], [accuracy, recall, precision, f1]):\n",
    "            th_metrics_calc[key].append(value)\n",
    "            \n",
    "    pooled_accuracy, pooled_recall, pooled_precision, pooled_f1 = th_metrics(\n",
    "        th_frequencies_pooled[\"tp\"], \n",
    "        th_frequencies_pooled[\"fp\"], \n",
    "        th_frequencies_pooled[\"tn\"], \n",
    "        th_frequencies_pooled[\"fn\"])\n",
    "    \n",
    "    mean_mse, std_mse             = mean(mse_calculated)\n",
    "    mean_accuracy, std_accuracy   = mean(th_metrics_calc[\"accuracy\"])\n",
    "    mean_recall, std_recall       = mean(th_metrics_calc[\"recall\"])\n",
    "    mean_precision, std_precision = mean(th_metrics_calc[\"precision\"])\n",
    "    mean_f1, std_f1               = mean(th_metrics_calc[\"f1\"])\n",
    "    \n",
    "    print(\"Pooled MSE: \", \"To Be Fixed\")\n",
    "    print(\"Pooled Accuracy: \", pooled_accuracy)\n",
    "    print(\"Pooled Recall: \", pooled_recall)\n",
    "    print(\"Pooled Precision: \", pooled_precision)\n",
    "    print(\"Pooled F1: \", pooled_f1)\n",
    "    print(\"Mean MSE = {0:.4} (std = {1:.4})\".format(mean_mse, std_mse))\n",
    "    print(\"Mean Accuracy = {0:.4} (std = {1:.4})\".format(mean_accuracy, std_accuracy))\n",
    "    print(\"Mean Recall = {0:.4} (std = {1:.4})\".format(mean_recall, std_recall))\n",
    "    print(\"Mean Precision = {0:.4} (std = {1:.4})\".format(mean_precision, std_precision))\n",
    "    print(\"Mean F1 = {0:.4} (std = {1:.4})\".format(mean_f1, std_f1))\n",
    "    \n",
    "    # some text format output\n",
    "    # keep track of bad examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(my_simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(my_diabolo_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show true box on image\n",
    "# show predicted heatmap on image\n",
    "# show true box on predicted heatmap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
